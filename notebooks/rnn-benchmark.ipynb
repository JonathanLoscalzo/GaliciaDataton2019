{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "import gc\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "from sklearn.model_selection import KFold\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('fivethirtyeight') \n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:3: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "data = pd.concat([\n",
    "       pd.read_csv(\"../input/pageviews/pageviews.csv\", parse_dates=[\"FEC_EVENT\"], usecols=[\"USER_ID\", \"FEC_EVENT\", \"PAGE\"]),\n",
    "       pd.read_csv(\"../input/pageviews_complemento/pageviews_complemento.csv\", parse_dates=[\"FEC_EVENT\"], usecols=[\"USER_ID\", \"FEC_EVENT\", \"PAGE\"])\n",
    "])\n",
    "\n",
    "data[\"day\"] = data.FEC_EVENT.dt.dayofyear - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_prev = pd.read_csv(\"../input/conversiones/conversiones.csv\")\n",
    "y_train = pd.Series(0, index=sorted(data.USER_ID.unique()))\n",
    "y_train.loc[y_prev[y_prev.mes >= 10].USER_ID.unique()] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Construcci√≥n de la sequencia de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((22870354, 4), 1733)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape, data.PAGE.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pages = data[data.FEC_EVENT.dt.month < 10].groupby(\"PAGE\").USER_ID.unique()\n",
    "# pages = pages.index[pages.apply(lambda x: y_train.loc[x].mean() / y_train.mean() - 1).abs() > 0.05]\n",
    "# data = data[data.PAGE.isin(pages)]\n",
    "# data.shape, data.PAGE.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pages = data.PAGE.value_counts()\n",
    "# pages = pages.index[pages < pages.iloc[int(pages.shape[0] * 0.1)]]\n",
    "# data = data[data.PAGE.isin(pages)]\n",
    "# data.shape, data.PAGE.nunique()                     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data[\"PAGE\"] = pd.factorize(data.PAGE)[0]\n",
    "npages = data.PAGE.max() + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = 60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataPrep(X, nusers, history, npages):\n",
    "    data = X.copy()\n",
    "    data[\"day\"] = data.day - data.day.max() + history - 1\n",
    "    data = data[data.day >= 0]\n",
    "    data = data.groupby([\"USER_ID\", \"day\", \"PAGE\"]).size().rename(\"cantidad\").reset_index().set_index(\"USER_ID\")\n",
    "    data = data.astype(np.int32)\n",
    "    res = np.zeros((nusers, history, npages), dtype=np.float32)\n",
    "    for user in range(nusers):\n",
    "        if user in data.index:\n",
    "            d = data.loc[user]\n",
    "            res[user, d.day, d.PAGE] = d.cantidad\n",
    "            res[user] /= (res[user].sum(axis=1) + 1e-16)[: , None]\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "96"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "try:\n",
    "    del X_train, X_test\n",
    "    gc.collect()\n",
    "except: pass\n",
    "\n",
    "X_train = dataPrep(data[data.FEC_EVENT.dt.month < 10], data.USER_ID.max() + 1, history, npages)\n",
    "X_test = dataPrep(data, data.USER_ID.max() + 1, history, npages)\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Model, Sequential\n",
    "from keras.layers import Input, Dense, CuDNNLSTM, add, concatenate, Dropout, Multiply, multiply\n",
    "from keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "from keras.engine.topology import Layer\n",
    "from keras import initializers, regularizers, constraints\n",
    "\n",
    "\n",
    "class Attention(Layer):\n",
    "    def __init__(self, step_dim,\n",
    "                 W_regularizer=None, b_regularizer=None,\n",
    "                 W_constraint=None, b_constraint=None,\n",
    "                 bias=True, **kwargs):\n",
    "        self.supports_masking = True\n",
    "        self.init = initializers.get('glorot_uniform')\n",
    "\n",
    "        self.W_regularizer = regularizers.get(W_regularizer)\n",
    "        self.b_regularizer = regularizers.get(b_regularizer)\n",
    "\n",
    "        self.W_constraint = constraints.get(W_constraint)\n",
    "        self.b_constraint = constraints.get(b_constraint)\n",
    "\n",
    "        self.bias = bias\n",
    "        self.step_dim = step_dim\n",
    "        self.features_dim = 0\n",
    "        super(Attention, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        assert len(input_shape) == 3\n",
    "\n",
    "        self.W = self.add_weight((input_shape[-1],),\n",
    "                                 initializer=self.init,\n",
    "                                 name='{}_W'.format(self.name),\n",
    "                                 regularizer=self.W_regularizer,\n",
    "                                 constraint=self.W_constraint)\n",
    "        self.features_dim = input_shape[-1]\n",
    "\n",
    "        if self.bias:\n",
    "            self.b = self.add_weight((input_shape[1],),\n",
    "                                     initializer='zero',\n",
    "                                     name='{}_b'.format(self.name),\n",
    "                                     regularizer=self.b_regularizer,\n",
    "                                     constraint=self.b_constraint)\n",
    "        else:\n",
    "            self.b = None\n",
    "\n",
    "        self.built = True\n",
    "\n",
    "    def compute_mask(self, input, input_mask=None):\n",
    "        return None\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        features_dim = self.features_dim\n",
    "        step_dim = self.step_dim\n",
    "\n",
    "        eij = K.reshape(K.dot(K.reshape(x, (-1, features_dim)),\n",
    "                        K.reshape(self.W, (features_dim, 1))), (-1, step_dim))\n",
    "\n",
    "        if self.bias:\n",
    "            eij += self.b\n",
    "\n",
    "        eij = K.tanh(eij)\n",
    "\n",
    "        a = K.exp(eij)\n",
    "\n",
    "        if mask is not None:\n",
    "            a *= K.cast(mask, K.floatx())\n",
    "\n",
    "        a /= K.cast(K.sum(a, axis=1, keepdims=True) + K.epsilon(), K.floatx())\n",
    "\n",
    "        a = K.expand_dims(a)\n",
    "        weighted_input = x * a\n",
    "        return K.sum(weighted_input, axis=1)\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape[0],  self.features_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Dense, Embedding, Input\n",
    "from keras.layers import CuDNNLSTM, Bidirectional, Dropout\n",
    "\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "def auroc(y_true, y_pred):\n",
    "    return tf.py_func(roc_auc_score, (y_true, y_pred), tf.double)\n",
    "\n",
    "def BidLstm(lstm_size=64):\n",
    "    inp = Input(shape=(history, npages))\n",
    "    x = Bidirectional(CuDNNLSTM(lstm_size, return_sequences=True))(inp)\n",
    "    x = Attention(history)(x)\n",
    "    x = Dense(lstm_size * 2, activation=\"relu\")(x)\n",
    "    x = Dropout(0.25)(x)\n",
    "    x = Dense(1, activation=\"sigmoid\")(x)\n",
    "    model = Model(inputs=inp, outputs=x)\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=[auroc])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 60, 1836)          0         \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 60, 128)           973824    \n",
      "_________________________________________________________________\n",
      "attention_1 (Attention)      (None, 128)               188       \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 990,653\n",
      "Trainable params: 990,653\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = BidLstm()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 10508 samples, validate on 1168 samples\n",
      "Epoch 1/1000\n",
      "10508/10508 [==============================] - 10s 939us/step - loss: 0.3335 - auroc: 0.4385 - val_loss: 0.1162 - val_auroc: 0.4246\n",
      "Epoch 2/1000\n",
      "10508/10508 [==============================] - 6s 597us/step - loss: 0.1564 - auroc: 0.5043 - val_loss: 0.1029 - val_auroc: 0.4684\n",
      "Epoch 3/1000\n",
      "10508/10508 [==============================] - 6s 597us/step - loss: 0.1526 - auroc: 0.5527 - val_loss: 0.0959 - val_auroc: 0.5253\n",
      "Epoch 4/1000\n",
      "10508/10508 [==============================] - 6s 600us/step - loss: 0.1532 - auroc: 0.5405 - val_loss: 0.0960 - val_auroc: 0.5604\n",
      "Epoch 5/1000\n",
      "10508/10508 [==============================] - 6s 601us/step - loss: 0.1515 - auroc: 0.5869 - val_loss: 0.0898 - val_auroc: 0.6957\n",
      "Epoch 6/1000\n",
      "10508/10508 [==============================] - 6s 599us/step - loss: 0.1485 - auroc: 0.6413 - val_loss: 0.0924 - val_auroc: 0.7493\n",
      "Epoch 7/1000\n",
      "10508/10508 [==============================] - 6s 602us/step - loss: 0.1394 - auroc: 0.7417 - val_loss: 0.0838 - val_auroc: 0.7406\n",
      "Epoch 8/1000\n",
      "10508/10508 [==============================] - 6s 601us/step - loss: 0.1341 - auroc: 0.7888 - val_loss: 0.0903 - val_auroc: 0.7470\n",
      "Epoch 9/1000\n",
      "10508/10508 [==============================] - 6s 606us/step - loss: 0.1281 - auroc: 0.8235 - val_loss: 0.0833 - val_auroc: 0.7281\n",
      "Epoch 10/1000\n",
      "10508/10508 [==============================] - 6s 603us/step - loss: 0.1280 - auroc: 0.8354 - val_loss: 0.0830 - val_auroc: 0.7275\n",
      "Epoch 11/1000\n",
      "10508/10508 [==============================] - 6s 604us/step - loss: 0.1240 - auroc: 0.8489 - val_loss: 0.0835 - val_auroc: 0.7399\n",
      "Epoch 12/1000\n",
      "10508/10508 [==============================] - 6s 609us/step - loss: 0.1247 - auroc: 0.8521 - val_loss: 0.0870 - val_auroc: 0.7521\n",
      "Epoch 13/1000\n",
      "10508/10508 [==============================] - 6s 603us/step - loss: 0.1202 - auroc: 0.8590 - val_loss: 0.0868 - val_auroc: 0.7330\n",
      "Epoch 14/1000\n",
      "10508/10508 [==============================] - 6s 602us/step - loss: 0.1197 - auroc: 0.8725 - val_loss: 0.0833 - val_auroc: 0.7405\n",
      "Epoch 15/1000\n",
      "10508/10508 [==============================] - 6s 602us/step - loss: 0.1165 - auroc: 0.8733 - val_loss: 0.0849 - val_auroc: 0.7251\n",
      "Epoch 16/1000\n",
      "10508/10508 [==============================] - 6s 604us/step - loss: 0.1188 - auroc: 0.8791 - val_loss: 0.0811 - val_auroc: 0.7441\n",
      "Epoch 17/1000\n",
      "10508/10508 [==============================] - 6s 607us/step - loss: 0.1151 - auroc: 0.8796 - val_loss: 0.0886 - val_auroc: 0.7339\n",
      "Epoch 18/1000\n",
      "10508/10508 [==============================] - 6s 607us/step - loss: 0.1156 - auroc: 0.8857 - val_loss: 0.0901 - val_auroc: 0.7068\n",
      "Epoch 19/1000\n",
      "10508/10508 [==============================] - 6s 605us/step - loss: 0.1131 - auroc: 0.8862 - val_loss: 0.0878 - val_auroc: 0.7282\n",
      "Epoch 20/1000\n",
      "10508/10508 [==============================] - 6s 603us/step - loss: 0.1110 - auroc: 0.8907 - val_loss: 0.0952 - val_auroc: 0.7010\n",
      "Epoch 21/1000\n",
      "10508/10508 [==============================] - 6s 603us/step - loss: 0.1131 - auroc: 0.8974 - val_loss: 0.0894 - val_auroc: 0.6997\n",
      "Epoch 22/1000\n",
      "10508/10508 [==============================] - 7s 636us/step - loss: 0.1146 - auroc: 0.8998 - val_loss: 0.0886 - val_auroc: 0.7242\n",
      "Restoring model weights from the end of the best epoch\n",
      "Epoch 00022: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f055e421a58>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " model.fit(X_train, y_train, batch_size=320, epochs=1000, verbose=1,\n",
    "              validation_split=0.1,\n",
    "              callbacks=[EarlyStopping(monitor='val_auroc', patience=10,\n",
    "                                       verbose=1, mode='max', restore_best_weights=True)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 10508 samples, validate on 1168 samples\n",
      "Epoch 1/1000\n",
      "10508/10508 [==============================] - 7s 695us/step - loss: 0.3365 - auroc: 0.4725 - val_loss: 0.1008 - val_auroc: 0.4127\n",
      "Epoch 2/1000\n",
      "10508/10508 [==============================] - 6s 603us/step - loss: 0.1596 - auroc: 0.4708 - val_loss: 0.0969 - val_auroc: 0.4488\n",
      "Epoch 3/1000\n",
      "10508/10508 [==============================] - 6s 603us/step - loss: 0.1545 - auroc: 0.5260 - val_loss: 0.0971 - val_auroc: 0.5038\n",
      "Epoch 4/1000\n",
      "10508/10508 [==============================] - 6s 601us/step - loss: 0.1549 - auroc: 0.5008 - val_loss: 0.0913 - val_auroc: 0.5539\n",
      "Epoch 5/1000\n",
      "10508/10508 [==============================] - 6s 602us/step - loss: 0.1517 - auroc: 0.5749 - val_loss: 0.0981 - val_auroc: 0.7100\n",
      "Epoch 6/1000\n",
      "10508/10508 [==============================] - 6s 602us/step - loss: 0.1494 - auroc: 0.6259 - val_loss: 0.0890 - val_auroc: 0.7274\n",
      "Epoch 7/1000\n",
      "10508/10508 [==============================] - 6s 600us/step - loss: 0.1355 - auroc: 0.7618 - val_loss: 0.0855 - val_auroc: 0.6929\n",
      "Epoch 8/1000\n",
      "10508/10508 [==============================] - 6s 603us/step - loss: 0.1382 - auroc: 0.7640 - val_loss: 0.0901 - val_auroc: 0.7185\n",
      "Epoch 9/1000\n",
      "10508/10508 [==============================] - 6s 606us/step - loss: 0.1315 - auroc: 0.7913 - val_loss: 0.0820 - val_auroc: 0.6872\n",
      "Epoch 10/1000\n",
      "10508/10508 [==============================] - 6s 609us/step - loss: 0.1253 - auroc: 0.8299 - val_loss: 0.0819 - val_auroc: 0.6986\n",
      "Epoch 11/1000\n",
      "10508/10508 [==============================] - 6s 600us/step - loss: 0.1239 - auroc: 0.8393 - val_loss: 0.0856 - val_auroc: 0.7480\n",
      "Epoch 12/1000\n",
      "10508/10508 [==============================] - 6s 600us/step - loss: 0.1237 - auroc: 0.8623 - val_loss: 0.0811 - val_auroc: 0.7412\n",
      "Epoch 13/1000\n",
      "10508/10508 [==============================] - 6s 601us/step - loss: 0.1209 - auroc: 0.8603 - val_loss: 0.0819 - val_auroc: 0.7145\n",
      "Epoch 14/1000\n",
      "10508/10508 [==============================] - 6s 604us/step - loss: 0.1227 - auroc: 0.8597 - val_loss: 0.0844 - val_auroc: 0.7441\n",
      "Epoch 15/1000\n",
      "10508/10508 [==============================] - 6s 608us/step - loss: 0.1201 - auroc: 0.8706 - val_loss: 0.0844 - val_auroc: 0.7082\n",
      "Epoch 16/1000\n",
      "10508/10508 [==============================] - 6s 604us/step - loss: 0.1178 - auroc: 0.8729 - val_loss: 0.0812 - val_auroc: 0.7174\n",
      "Epoch 17/1000\n",
      "10508/10508 [==============================] - 6s 604us/step - loss: 0.1193 - auroc: 0.8696 - val_loss: 0.0810 - val_auroc: 0.7550\n",
      "Epoch 18/1000\n",
      "10508/10508 [==============================] - 6s 603us/step - loss: 0.1202 - auroc: 0.8681 - val_loss: 0.0868 - val_auroc: 0.7393\n",
      "Epoch 19/1000\n",
      "10508/10508 [==============================] - 6s 605us/step - loss: 0.1167 - auroc: 0.8707 - val_loss: 0.0849 - val_auroc: 0.7406\n",
      "Epoch 20/1000\n",
      "10508/10508 [==============================] - 6s 607us/step - loss: 0.1169 - auroc: 0.8773 - val_loss: 0.0912 - val_auroc: 0.7170\n",
      "Epoch 21/1000\n",
      "10508/10508 [==============================] - 6s 604us/step - loss: 0.1159 - auroc: 0.8758 - val_loss: 0.0871 - val_auroc: 0.7292\n",
      "Epoch 22/1000\n",
      "10508/10508 [==============================] - 6s 605us/step - loss: 0.1147 - auroc: 0.8905 - val_loss: 0.0871 - val_auroc: 0.7266\n",
      "Epoch 23/1000\n",
      "10508/10508 [==============================] - 6s 606us/step - loss: 0.1164 - auroc: 0.8872 - val_loss: 0.0861 - val_auroc: 0.7331\n",
      "Epoch 24/1000\n",
      "10508/10508 [==============================] - 6s 606us/step - loss: 0.1118 - auroc: 0.8873 - val_loss: 0.0868 - val_auroc: 0.7176\n",
      "Epoch 25/1000\n",
      "10508/10508 [==============================] - 6s 607us/step - loss: 0.1124 - auroc: 0.8970 - val_loss: 0.0905 - val_auroc: 0.7271\n",
      "Epoch 26/1000\n",
      "10508/10508 [==============================] - 6s 607us/step - loss: 0.1123 - auroc: 0.8992 - val_loss: 0.0917 - val_auroc: 0.7324\n",
      "Epoch 27/1000\n",
      "10508/10508 [==============================] - 6s 608us/step - loss: 0.1118 - auroc: 0.8939 - val_loss: 0.0907 - val_auroc: 0.7330\n",
      "Restoring model weights from the end of the best epoch\n",
      "Epoch 00027: early stopping\n",
      "Train on 10508 samples, validate on 1168 samples\n",
      "Epoch 1/1000\n",
      "10508/10508 [==============================] - 7s 706us/step - loss: 0.3310 - auroc: 0.4426 - val_loss: 0.1196 - val_auroc: 0.4189\n",
      "Epoch 2/1000\n",
      "10508/10508 [==============================] - 6s 598us/step - loss: 0.1581 - auroc: 0.4848 - val_loss: 0.1015 - val_auroc: 0.4555\n",
      "Epoch 3/1000\n",
      "10508/10508 [==============================] - 6s 598us/step - loss: 0.1541 - auroc: 0.4953 - val_loss: 0.0909 - val_auroc: 0.4997\n",
      "Epoch 4/1000\n",
      "10508/10508 [==============================] - 6s 604us/step - loss: 0.1540 - auroc: 0.5309 - val_loss: 0.0961 - val_auroc: 0.5436\n",
      "Epoch 5/1000\n",
      "10508/10508 [==============================] - 6s 603us/step - loss: 0.1508 - auroc: 0.5969 - val_loss: 0.0983 - val_auroc: 0.6168\n",
      "Epoch 6/1000\n",
      "10508/10508 [==============================] - 6s 607us/step - loss: 0.1484 - auroc: 0.6322 - val_loss: 0.1006 - val_auroc: 0.7209\n",
      "Epoch 7/1000\n",
      "10508/10508 [==============================] - 6s 605us/step - loss: 0.1422 - auroc: 0.7107 - val_loss: 0.0854 - val_auroc: 0.7125\n",
      "Epoch 8/1000\n",
      "10508/10508 [==============================] - 6s 604us/step - loss: 0.1366 - auroc: 0.7619 - val_loss: 0.0863 - val_auroc: 0.7475\n",
      "Epoch 9/1000\n",
      "10508/10508 [==============================] - 6s 606us/step - loss: 0.1284 - auroc: 0.8061 - val_loss: 0.0821 - val_auroc: 0.7360\n",
      "Epoch 10/1000\n",
      "10508/10508 [==============================] - 6s 603us/step - loss: 0.1261 - auroc: 0.8294 - val_loss: 0.0860 - val_auroc: 0.7329\n",
      "Epoch 11/1000\n",
      "10508/10508 [==============================] - 6s 605us/step - loss: 0.1265 - auroc: 0.8457 - val_loss: 0.0800 - val_auroc: 0.7344\n",
      "Epoch 12/1000\n",
      "10508/10508 [==============================] - 6s 608us/step - loss: 0.1262 - auroc: 0.8522 - val_loss: 0.0910 - val_auroc: 0.7150\n",
      "Epoch 13/1000\n",
      "10508/10508 [==============================] - 6s 605us/step - loss: 0.1238 - auroc: 0.8618 - val_loss: 0.0800 - val_auroc: 0.7024\n",
      "Epoch 14/1000\n",
      "10508/10508 [==============================] - 6s 605us/step - loss: 0.1214 - auroc: 0.8591 - val_loss: 0.0812 - val_auroc: 0.7397\n",
      "Epoch 15/1000\n",
      "10508/10508 [==============================] - 6s 606us/step - loss: 0.1196 - auroc: 0.8692 - val_loss: 0.0891 - val_auroc: 0.7492\n",
      "Epoch 16/1000\n",
      "10508/10508 [==============================] - 6s 607us/step - loss: 0.1217 - auroc: 0.8662 - val_loss: 0.0843 - val_auroc: 0.7449\n",
      "Epoch 17/1000\n",
      "10508/10508 [==============================] - 6s 608us/step - loss: 0.1193 - auroc: 0.8716 - val_loss: 0.0847 - val_auroc: 0.7445\n",
      "Epoch 18/1000\n",
      "10508/10508 [==============================] - 6s 607us/step - loss: 0.1163 - auroc: 0.8739 - val_loss: 0.0842 - val_auroc: 0.7567\n",
      "Epoch 19/1000\n",
      "10508/10508 [==============================] - 7s 636us/step - loss: 0.1201 - auroc: 0.8780 - val_loss: 0.0858 - val_auroc: 0.7527\n",
      "Epoch 20/1000\n",
      "10508/10508 [==============================] - 6s 617us/step - loss: 0.1157 - auroc: 0.8852 - val_loss: 0.0893 - val_auroc: 0.7437\n",
      "Epoch 21/1000\n",
      "10508/10508 [==============================] - 6s 608us/step - loss: 0.1158 - auroc: 0.8900 - val_loss: 0.0862 - val_auroc: 0.7477\n",
      "Epoch 22/1000\n",
      "10508/10508 [==============================] - 6s 608us/step - loss: 0.1123 - auroc: 0.8930 - val_loss: 0.0885 - val_auroc: 0.7250\n",
      "Epoch 23/1000\n",
      "10508/10508 [==============================] - 6s 609us/step - loss: 0.1139 - auroc: 0.8939 - val_loss: 0.0881 - val_auroc: 0.7237\n",
      "Epoch 24/1000\n",
      "10508/10508 [==============================] - 6s 609us/step - loss: 0.1137 - auroc: 0.8929 - val_loss: 0.0923 - val_auroc: 0.7204\n",
      "Epoch 25/1000\n",
      "10508/10508 [==============================] - 6s 613us/step - loss: 0.1103 - auroc: 0.8985 - val_loss: 0.0916 - val_auroc: 0.7203\n",
      "Epoch 26/1000\n",
      "10508/10508 [==============================] - 6s 610us/step - loss: 0.1086 - auroc: 0.9005 - val_loss: 0.0902 - val_auroc: 0.7102\n",
      "Epoch 27/1000\n",
      "10508/10508 [==============================] - 6s 608us/step - loss: 0.1116 - auroc: 0.8974 - val_loss: 0.0911 - val_auroc: 0.7185\n",
      "Epoch 28/1000\n",
      "10508/10508 [==============================] - 6s 609us/step - loss: 0.1084 - auroc: 0.9013 - val_loss: 0.0977 - val_auroc: 0.7076\n",
      "Restoring model weights from the end of the best epoch\n",
      "Epoch 00028: early stopping\n",
      "Train on 10508 samples, validate on 1168 samples\n",
      "Epoch 1/1000\n",
      "10508/10508 [==============================] - 8s 737us/step - loss: 0.3279 - auroc: 0.4643 - val_loss: 0.1165 - val_auroc: 0.4084\n",
      "Epoch 2/1000\n",
      "10508/10508 [==============================] - 6s 610us/step - loss: 0.1567 - auroc: 0.4850 - val_loss: 0.1062 - val_auroc: 0.4514\n",
      "Epoch 3/1000\n",
      "10508/10508 [==============================] - 6s 607us/step - loss: 0.1541 - auroc: 0.5377 - val_loss: 0.0980 - val_auroc: 0.5127\n",
      "Epoch 4/1000\n",
      "10508/10508 [==============================] - 6s 608us/step - loss: 0.1519 - auroc: 0.5543 - val_loss: 0.1040 - val_auroc: 0.5818\n",
      "Epoch 5/1000\n",
      "10508/10508 [==============================] - 6s 606us/step - loss: 0.1495 - auroc: 0.6206 - val_loss: 0.0916 - val_auroc: 0.7130\n",
      "Epoch 6/1000\n",
      "10508/10508 [==============================] - 6s 606us/step - loss: 0.1443 - auroc: 0.7061 - val_loss: 0.0820 - val_auroc: 0.7384\n",
      "Epoch 7/1000\n",
      "10508/10508 [==============================] - 6s 607us/step - loss: 0.1325 - auroc: 0.7956 - val_loss: 0.0829 - val_auroc: 0.7467\n",
      "Epoch 8/1000\n",
      "10508/10508 [==============================] - 6s 612us/step - loss: 0.1299 - auroc: 0.8137 - val_loss: 0.0836 - val_auroc: 0.7551\n",
      "Epoch 9/1000\n",
      "10508/10508 [==============================] - 6s 610us/step - loss: 0.1240 - auroc: 0.8375 - val_loss: 0.0833 - val_auroc: 0.7417\n",
      "Epoch 10/1000\n",
      "10508/10508 [==============================] - 6s 612us/step - loss: 0.1260 - auroc: 0.8458 - val_loss: 0.0819 - val_auroc: 0.7316\n",
      "Epoch 11/1000\n",
      "10508/10508 [==============================] - 6s 611us/step - loss: 0.1226 - auroc: 0.8571 - val_loss: 0.0831 - val_auroc: 0.7520\n",
      "Epoch 12/1000\n",
      "10508/10508 [==============================] - 6s 609us/step - loss: 0.1213 - auroc: 0.8634 - val_loss: 0.1037 - val_auroc: 0.7348\n",
      "Epoch 13/1000\n",
      "10508/10508 [==============================] - 6s 609us/step - loss: 0.1212 - auroc: 0.8729 - val_loss: 0.0809 - val_auroc: 0.7335\n",
      "Epoch 14/1000\n",
      "10508/10508 [==============================] - 6s 612us/step - loss: 0.1180 - auroc: 0.8698 - val_loss: 0.0824 - val_auroc: 0.7094\n",
      "Epoch 15/1000\n",
      "10508/10508 [==============================] - 6s 612us/step - loss: 0.1190 - auroc: 0.8762 - val_loss: 0.0855 - val_auroc: 0.7470\n",
      "Epoch 16/1000\n",
      "10508/10508 [==============================] - 6s 611us/step - loss: 0.1177 - auroc: 0.8781 - val_loss: 0.0836 - val_auroc: 0.7451\n",
      "Epoch 17/1000\n",
      "10508/10508 [==============================] - 6s 611us/step - loss: 0.1148 - auroc: 0.8817 - val_loss: 0.0844 - val_auroc: 0.7197\n",
      "Epoch 18/1000\n",
      "10508/10508 [==============================] - 6s 611us/step - loss: 0.1134 - auroc: 0.8860 - val_loss: 0.0876 - val_auroc: 0.7144\n",
      "Restoring model weights from the end of the best epoch\n",
      "Epoch 00018: early stopping\n",
      "Train on 10508 samples, validate on 1168 samples\n",
      "Epoch 1/1000\n",
      "10508/10508 [==============================] - 8s 753us/step - loss: 0.3357 - auroc: 0.4896 - val_loss: 0.1099 - val_auroc: 0.4104\n",
      "Epoch 2/1000\n",
      "10508/10508 [==============================] - 6s 612us/step - loss: 0.1584 - auroc: 0.4620 - val_loss: 0.1020 - val_auroc: 0.4540\n",
      "Epoch 3/1000\n",
      "10508/10508 [==============================] - 6s 607us/step - loss: 0.1552 - auroc: 0.4882 - val_loss: 0.0972 - val_auroc: 0.5045\n",
      "Epoch 4/1000\n",
      "10508/10508 [==============================] - 6s 609us/step - loss: 0.1535 - auroc: 0.5312 - val_loss: 0.1033 - val_auroc: 0.5660\n",
      "Epoch 5/1000\n",
      "10508/10508 [==============================] - 6s 607us/step - loss: 0.1515 - auroc: 0.5577 - val_loss: 0.1019 - val_auroc: 0.7217\n",
      "Epoch 6/1000\n",
      "10508/10508 [==============================] - 6s 611us/step - loss: 0.1465 - auroc: 0.6671 - val_loss: 0.0841 - val_auroc: 0.7693\n",
      "Epoch 7/1000\n",
      "10508/10508 [==============================] - 6s 609us/step - loss: 0.1406 - auroc: 0.7406 - val_loss: 0.0828 - val_auroc: 0.7455\n",
      "Epoch 8/1000\n",
      "10508/10508 [==============================] - 6s 610us/step - loss: 0.1344 - auroc: 0.7770 - val_loss: 0.0825 - val_auroc: 0.7310\n",
      "Epoch 9/1000\n",
      "10508/10508 [==============================] - 6s 608us/step - loss: 0.1350 - auroc: 0.7867 - val_loss: 0.0839 - val_auroc: 0.7186\n",
      "Epoch 10/1000\n",
      "10508/10508 [==============================] - 6s 607us/step - loss: 0.1282 - auroc: 0.8274 - val_loss: 0.1040 - val_auroc: 0.7227\n",
      "Epoch 11/1000\n",
      "10508/10508 [==============================] - 6s 609us/step - loss: 0.1268 - auroc: 0.8334 - val_loss: 0.0817 - val_auroc: 0.7319\n",
      "Epoch 12/1000\n",
      "10508/10508 [==============================] - 6s 609us/step - loss: 0.1219 - auroc: 0.8500 - val_loss: 0.0839 - val_auroc: 0.7285\n",
      "Epoch 13/1000\n",
      "10508/10508 [==============================] - 6s 610us/step - loss: 0.1204 - auroc: 0.8616 - val_loss: 0.0830 - val_auroc: 0.7368\n",
      "Epoch 14/1000\n",
      "10508/10508 [==============================] - 6s 613us/step - loss: 0.1203 - auroc: 0.8685 - val_loss: 0.0828 - val_auroc: 0.7408\n",
      "Epoch 15/1000\n",
      "10508/10508 [==============================] - 6s 612us/step - loss: 0.1177 - auroc: 0.8711 - val_loss: 0.0854 - val_auroc: 0.7380\n",
      "Epoch 16/1000\n",
      "10508/10508 [==============================] - 6s 609us/step - loss: 0.1194 - auroc: 0.8718 - val_loss: 0.0870 - val_auroc: 0.7469\n",
      "Restoring model weights from the end of the best epoch\n",
      "Epoch 00016: early stopping\n",
      "Train on 10508 samples, validate on 1168 samples\n",
      "Epoch 1/1000\n",
      "10508/10508 [==============================] - 8s 803us/step - loss: 0.3328 - auroc: 0.4637 - val_loss: 0.1105 - val_auroc: 0.4323\n",
      "Epoch 2/1000\n",
      "10508/10508 [==============================] - 6s 615us/step - loss: 0.1573 - auroc: 0.4810 - val_loss: 0.0968 - val_auroc: 0.4661\n",
      "Epoch 3/1000\n",
      "10508/10508 [==============================] - 6s 612us/step - loss: 0.1544 - auroc: 0.5017 - val_loss: 0.0956 - val_auroc: 0.5155\n",
      "Epoch 4/1000\n",
      "10508/10508 [==============================] - 6s 608us/step - loss: 0.1538 - auroc: 0.5237 - val_loss: 0.0920 - val_auroc: 0.5470\n",
      "Epoch 5/1000\n",
      "10508/10508 [==============================] - 6s 611us/step - loss: 0.1517 - auroc: 0.5787 - val_loss: 0.0994 - val_auroc: 0.6928\n",
      "Epoch 6/1000\n",
      "10508/10508 [==============================] - 6s 614us/step - loss: 0.1493 - auroc: 0.6175 - val_loss: 0.0937 - val_auroc: 0.7496\n",
      "Epoch 7/1000\n",
      "10508/10508 [==============================] - 6s 611us/step - loss: 0.1387 - auroc: 0.7561 - val_loss: 0.0839 - val_auroc: 0.7433\n",
      "Epoch 8/1000\n",
      "10508/10508 [==============================] - 6s 610us/step - loss: 0.1328 - auroc: 0.7783 - val_loss: 0.0841 - val_auroc: 0.6911\n",
      "Epoch 9/1000\n",
      "10508/10508 [==============================] - 6s 611us/step - loss: 0.1297 - auroc: 0.8287 - val_loss: 0.0847 - val_auroc: 0.7258\n",
      "Epoch 10/1000\n",
      "10508/10508 [==============================] - 6s 612us/step - loss: 0.1298 - auroc: 0.8296 - val_loss: 0.0846 - val_auroc: 0.7381\n",
      "Epoch 11/1000\n",
      "10508/10508 [==============================] - 6s 611us/step - loss: 0.1236 - auroc: 0.8441 - val_loss: 0.0867 - val_auroc: 0.7554\n",
      "Epoch 12/1000\n",
      "10508/10508 [==============================] - 6s 610us/step - loss: 0.1253 - auroc: 0.8473 - val_loss: 0.0826 - val_auroc: 0.7493\n",
      "Epoch 13/1000\n",
      "10508/10508 [==============================] - 6s 618us/step - loss: 0.1204 - auroc: 0.8579 - val_loss: 0.0820 - val_auroc: 0.7614\n",
      "Epoch 14/1000\n",
      "10508/10508 [==============================] - 6s 615us/step - loss: 0.1174 - auroc: 0.8711 - val_loss: 0.0922 - val_auroc: 0.7532\n",
      "Epoch 15/1000\n",
      "10508/10508 [==============================] - 6s 611us/step - loss: 0.1232 - auroc: 0.8602 - val_loss: 0.0983 - val_auroc: 0.7403\n",
      "Epoch 16/1000\n",
      "10508/10508 [==============================] - 6s 611us/step - loss: 0.1213 - auroc: 0.8658 - val_loss: 0.0831 - val_auroc: 0.7442\n",
      "Epoch 17/1000\n",
      "10508/10508 [==============================] - 6s 612us/step - loss: 0.1162 - auroc: 0.8764 - val_loss: 0.0858 - val_auroc: 0.7455\n",
      "Epoch 18/1000\n",
      "10508/10508 [==============================] - 6s 611us/step - loss: 0.1163 - auroc: 0.8787 - val_loss: 0.0827 - val_auroc: 0.7502\n",
      "Epoch 19/1000\n",
      "10508/10508 [==============================] - 6s 611us/step - loss: 0.1148 - auroc: 0.8795 - val_loss: 0.0858 - val_auroc: 0.7394\n",
      "Epoch 20/1000\n",
      "10508/10508 [==============================] - 6s 612us/step - loss: 0.1132 - auroc: 0.8852 - val_loss: 0.0863 - val_auroc: 0.7460\n",
      "Epoch 21/1000\n",
      "10508/10508 [==============================] - 6s 615us/step - loss: 0.1149 - auroc: 0.8915 - val_loss: 0.0865 - val_auroc: 0.7449\n",
      "Epoch 22/1000\n",
      "10508/10508 [==============================] - 6s 619us/step - loss: 0.1134 - auroc: 0.8907 - val_loss: 0.0845 - val_auroc: 0.7241\n",
      "Epoch 23/1000\n",
      "10508/10508 [==============================] - 6s 615us/step - loss: 0.1137 - auroc: 0.8915 - val_loss: 0.0884 - val_auroc: 0.7095\n",
      "Restoring model weights from the end of the best epoch\n",
      "Epoch 00023: early stopping\n",
      "Train on 10508 samples, validate on 1168 samples\n",
      "Epoch 1/1000\n",
      "10508/10508 [==============================] - 8s 798us/step - loss: 0.3145 - auroc: 0.4527 - val_loss: 0.1135 - val_auroc: 0.4169\n",
      "Epoch 2/1000\n",
      "10508/10508 [==============================] - 6s 607us/step - loss: 0.1568 - auroc: 0.4802 - val_loss: 0.0969 - val_auroc: 0.4613\n",
      "Epoch 3/1000\n",
      "10508/10508 [==============================] - 6s 608us/step - loss: 0.1531 - auroc: 0.5498 - val_loss: 0.0982 - val_auroc: 0.5196\n",
      "Epoch 4/1000\n",
      "10508/10508 [==============================] - 6s 606us/step - loss: 0.1510 - auroc: 0.5833 - val_loss: 0.0978 - val_auroc: 0.6327\n",
      "Epoch 5/1000\n",
      "10508/10508 [==============================] - 6s 609us/step - loss: 0.1476 - auroc: 0.6449 - val_loss: 0.0836 - val_auroc: 0.7356\n",
      "Epoch 6/1000\n",
      "10508/10508 [==============================] - 6s 608us/step - loss: 0.1369 - auroc: 0.7567 - val_loss: 0.0832 - val_auroc: 0.7547\n",
      "Epoch 7/1000\n",
      "10508/10508 [==============================] - 6s 610us/step - loss: 0.1333 - auroc: 0.7853 - val_loss: 0.0891 - val_auroc: 0.7667\n",
      "Epoch 8/1000\n",
      "10508/10508 [==============================] - 6s 609us/step - loss: 0.1270 - auroc: 0.8265 - val_loss: 0.0879 - val_auroc: 0.7649\n",
      "Epoch 9/1000\n",
      "10508/10508 [==============================] - 6s 611us/step - loss: 0.1254 - auroc: 0.8451 - val_loss: 0.0840 - val_auroc: 0.7330\n",
      "Epoch 10/1000\n",
      "10508/10508 [==============================] - 6s 616us/step - loss: 0.1253 - auroc: 0.8529 - val_loss: 0.0812 - val_auroc: 0.7412\n",
      "Epoch 11/1000\n",
      "10508/10508 [==============================] - 6s 614us/step - loss: 0.1190 - auroc: 0.8628 - val_loss: 0.0837 - val_auroc: 0.7418\n",
      "Epoch 12/1000\n",
      "10508/10508 [==============================] - 6s 613us/step - loss: 0.1197 - auroc: 0.8703 - val_loss: 0.0824 - val_auroc: 0.7492\n",
      "Epoch 13/1000\n",
      "10508/10508 [==============================] - 6s 613us/step - loss: 0.1156 - auroc: 0.8767 - val_loss: 0.0846 - val_auroc: 0.7366\n",
      "Epoch 14/1000\n",
      "10508/10508 [==============================] - 6s 612us/step - loss: 0.1161 - auroc: 0.8753 - val_loss: 0.0870 - val_auroc: 0.7516\n",
      "Epoch 15/1000\n",
      "10508/10508 [==============================] - 6s 613us/step - loss: 0.1190 - auroc: 0.8841 - val_loss: 0.0848 - val_auroc: 0.7455\n",
      "Epoch 16/1000\n",
      "10508/10508 [==============================] - 6s 613us/step - loss: 0.1146 - auroc: 0.8892 - val_loss: 0.0943 - val_auroc: 0.7416\n",
      "Epoch 17/1000\n",
      "10508/10508 [==============================] - 6s 611us/step - loss: 0.1141 - auroc: 0.8931 - val_loss: 0.0896 - val_auroc: 0.7186\n",
      "Restoring model weights from the end of the best epoch\n",
      "Epoch 00017: early stopping\n",
      "Train on 10508 samples, validate on 1168 samples\n",
      "Epoch 1/1000\n",
      "10508/10508 [==============================] - 9s 815us/step - loss: 0.3301 - auroc: 0.4654 - val_loss: 0.1139 - val_auroc: 0.4115\n",
      "Epoch 2/1000\n",
      "10508/10508 [==============================] - 6s 606us/step - loss: 0.1561 - auroc: 0.4849 - val_loss: 0.1067 - val_auroc: 0.4592\n",
      "Epoch 3/1000\n",
      "10508/10508 [==============================] - 6s 608us/step - loss: 0.1543 - auroc: 0.5071 - val_loss: 0.0961 - val_auroc: 0.5192\n",
      "Epoch 4/1000\n",
      "10508/10508 [==============================] - 6s 610us/step - loss: 0.1523 - auroc: 0.5538 - val_loss: 0.0976 - val_auroc: 0.5781\n",
      "Epoch 5/1000\n",
      "10508/10508 [==============================] - 6s 613us/step - loss: 0.1501 - auroc: 0.6124 - val_loss: 0.0939 - val_auroc: 0.7270\n",
      "Epoch 6/1000\n",
      "10508/10508 [==============================] - 7s 642us/step - loss: 0.1421 - auroc: 0.7132 - val_loss: 0.0819 - val_auroc: 0.7640\n",
      "Epoch 7/1000\n",
      "10508/10508 [==============================] - 6s 612us/step - loss: 0.1424 - auroc: 0.7505 - val_loss: 0.0969 - val_auroc: 0.7469\n",
      "Epoch 8/1000\n",
      "10508/10508 [==============================] - 6s 610us/step - loss: 0.1354 - auroc: 0.7827 - val_loss: 0.0821 - val_auroc: 0.7343\n",
      "Epoch 9/1000\n",
      "10508/10508 [==============================] - 6s 608us/step - loss: 0.1348 - auroc: 0.8118 - val_loss: 0.0827 - val_auroc: 0.7353\n",
      "Epoch 10/1000\n",
      "10508/10508 [==============================] - 6s 611us/step - loss: 0.1285 - auroc: 0.8180 - val_loss: 0.0898 - val_auroc: 0.7206\n",
      "Epoch 11/1000\n",
      "10508/10508 [==============================] - 6s 609us/step - loss: 0.1275 - auroc: 0.8327 - val_loss: 0.0920 - val_auroc: 0.7238\n",
      "Epoch 12/1000\n",
      "10508/10508 [==============================] - 6s 609us/step - loss: 0.1243 - auroc: 0.8389 - val_loss: 0.0858 - val_auroc: 0.7293\n",
      "Epoch 13/1000\n",
      "10508/10508 [==============================] - 6s 608us/step - loss: 0.1252 - auroc: 0.8585 - val_loss: 0.0828 - val_auroc: 0.7311\n",
      "Epoch 14/1000\n",
      "10508/10508 [==============================] - 6s 610us/step - loss: 0.1188 - auroc: 0.8635 - val_loss: 0.0861 - val_auroc: 0.7290\n",
      "Epoch 15/1000\n",
      "10508/10508 [==============================] - 6s 610us/step - loss: 0.1180 - auroc: 0.8698 - val_loss: 0.0836 - val_auroc: 0.7300\n",
      "Epoch 16/1000\n",
      "10508/10508 [==============================] - 6s 611us/step - loss: 0.1180 - auroc: 0.8736 - val_loss: 0.0888 - val_auroc: 0.7403\n",
      "Restoring model weights from the end of the best epoch\n",
      "Epoch 00016: early stopping\n",
      "Train on 10508 samples, validate on 1168 samples\n",
      "Epoch 1/1000\n",
      "10508/10508 [==============================] - 9s 842us/step - loss: 0.3405 - auroc: 0.4554 - val_loss: 0.1111 - val_auroc: 0.4139\n",
      "Epoch 2/1000\n",
      "10508/10508 [==============================] - 6s 613us/step - loss: 0.1588 - auroc: 0.4623 - val_loss: 0.0977 - val_auroc: 0.4536\n",
      "Epoch 3/1000\n",
      "10508/10508 [==============================] - 6s 611us/step - loss: 0.1544 - auroc: 0.5046 - val_loss: 0.0976 - val_auroc: 0.5062\n",
      "Epoch 4/1000\n",
      "10508/10508 [==============================] - 6s 610us/step - loss: 0.1529 - auroc: 0.5404 - val_loss: 0.0962 - val_auroc: 0.5525\n",
      "Epoch 5/1000\n",
      "10508/10508 [==============================] - 6s 610us/step - loss: 0.1515 - auroc: 0.5750 - val_loss: 0.0988 - val_auroc: 0.6836\n",
      "Epoch 6/1000\n",
      "10508/10508 [==============================] - 6s 611us/step - loss: 0.1473 - auroc: 0.6461 - val_loss: 0.0872 - val_auroc: 0.7647\n",
      "Epoch 7/1000\n",
      "10508/10508 [==============================] - 6s 610us/step - loss: 0.1411 - auroc: 0.7309 - val_loss: 0.0846 - val_auroc: 0.7370\n",
      "Epoch 8/1000\n",
      "10508/10508 [==============================] - 6s 609us/step - loss: 0.1331 - auroc: 0.7819 - val_loss: 0.0954 - val_auroc: 0.7330\n",
      "Epoch 9/1000\n",
      "10508/10508 [==============================] - 6s 615us/step - loss: 0.1263 - auroc: 0.8223 - val_loss: 0.0814 - val_auroc: 0.7258\n",
      "Epoch 10/1000\n",
      "10508/10508 [==============================] - 6s 615us/step - loss: 0.1268 - auroc: 0.8410 - val_loss: 0.0865 - val_auroc: 0.7334\n",
      "Epoch 11/1000\n",
      "10508/10508 [==============================] - 6s 614us/step - loss: 0.1269 - auroc: 0.8471 - val_loss: 0.0825 - val_auroc: 0.7396\n",
      "Epoch 12/1000\n",
      "10508/10508 [==============================] - 6s 615us/step - loss: 0.1231 - auroc: 0.8445 - val_loss: 0.0878 - val_auroc: 0.7325\n",
      "Epoch 13/1000\n",
      "10508/10508 [==============================] - 6s 610us/step - loss: 0.1212 - auroc: 0.8437 - val_loss: 0.0810 - val_auroc: 0.7422\n",
      "Epoch 14/1000\n",
      "10508/10508 [==============================] - 6s 608us/step - loss: 0.1181 - auroc: 0.8694 - val_loss: 0.0811 - val_auroc: 0.7435\n",
      "Epoch 15/1000\n",
      "10508/10508 [==============================] - 6s 611us/step - loss: 0.1198 - auroc: 0.8678 - val_loss: 0.0826 - val_auroc: 0.7428\n",
      "Epoch 16/1000\n",
      "10508/10508 [==============================] - 6s 612us/step - loss: 0.1223 - auroc: 0.8765 - val_loss: 0.0820 - val_auroc: 0.7432\n",
      "Restoring model weights from the end of the best epoch\n",
      "Epoch 00016: early stopping\n",
      "Train on 10508 samples, validate on 1168 samples\n",
      "Epoch 1/1000\n",
      "10508/10508 [==============================] - 9s 853us/step - loss: 0.3213 - auroc: 0.4735 - val_loss: 0.1095 - val_auroc: 0.4175\n",
      "Epoch 2/1000\n",
      "10508/10508 [==============================] - 6s 608us/step - loss: 0.1584 - auroc: 0.4664 - val_loss: 0.1033 - val_auroc: 0.4590\n",
      "Epoch 3/1000\n",
      "10508/10508 [==============================] - 6s 610us/step - loss: 0.1546 - auroc: 0.5030 - val_loss: 0.0945 - val_auroc: 0.5220\n",
      "Epoch 4/1000\n",
      "10508/10508 [==============================] - 6s 616us/step - loss: 0.1524 - auroc: 0.5585 - val_loss: 0.0969 - val_auroc: 0.5701\n",
      "Epoch 5/1000\n",
      "10508/10508 [==============================] - 6s 610us/step - loss: 0.1514 - auroc: 0.5830 - val_loss: 0.0914 - val_auroc: 0.7301\n",
      "Epoch 6/1000\n",
      "10508/10508 [==============================] - 6s 610us/step - loss: 0.1485 - auroc: 0.6261 - val_loss: 0.0855 - val_auroc: 0.7685\n",
      "Epoch 7/1000\n",
      "10508/10508 [==============================] - 6s 611us/step - loss: 0.1404 - auroc: 0.7522 - val_loss: 0.0810 - val_auroc: 0.7654\n",
      "Epoch 8/1000\n",
      "10508/10508 [==============================] - 6s 612us/step - loss: 0.1336 - auroc: 0.7848 - val_loss: 0.0857 - val_auroc: 0.7410\n",
      "Epoch 9/1000\n",
      "10508/10508 [==============================] - 6s 611us/step - loss: 0.1295 - auroc: 0.8075 - val_loss: 0.0815 - val_auroc: 0.7309\n",
      "Epoch 10/1000\n",
      "10508/10508 [==============================] - 6s 610us/step - loss: 0.1251 - auroc: 0.8316 - val_loss: 0.0880 - val_auroc: 0.7352\n",
      "Epoch 11/1000\n",
      "10508/10508 [==============================] - 6s 612us/step - loss: 0.1241 - auroc: 0.8538 - val_loss: 0.0881 - val_auroc: 0.7306\n",
      "Epoch 12/1000\n",
      " 2880/10508 [=======>......................] - ETA: 4s - loss: 0.1259 - auroc: 0.8334"
     ]
    }
   ],
   "source": [
    "test_probs = []\n",
    "for i in range(10):\n",
    "    model = model = BidLstm()\n",
    "    model.fit(X_train, y_train, batch_size=320, epochs=1000, verbose=1,\n",
    "              validation_split=0.1,\n",
    "              callbacks=[EarlyStopping(monitor='val_auroc', patience=10,\n",
    "                                       verbose=1, mode='max', restore_best_weights=True)])\n",
    "    \n",
    "    test_probs.append(pd.Series(model.predict(X_test)[:, -1], name=\"fold_\" + str(i)))\n",
    "\n",
    "test_probs = pd.concat(test_probs, axis=1).mean(axis=1)\n",
    "test_probs.index.name=\"USER_ID\"\n",
    "test_probs.name=\"SCORE\"\n",
    "\n",
    "test_probs.to_csv(\"rnn_benchmark.zip\", header=True, compression=\"zip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
